{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "pdist = nn.PairwiseDistance(p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## batch norm\n",
    "按通道维度计算平均值和方差\n",
    "$$\n",
    "y=\\frac{x-E(x)}{\\sqrt{Var(x)+\\epsilon}}*\\gamma+\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.8286, grad_fn=<SumBackward0>)\n",
      "tensor(2.0101, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand([16,32,64,64])\n",
    "# print(inputs)\n",
    "mean = torch.mean(inputs, [0,2,3], keepdim=True) # 按通道维度即32那个维度，计算那个维度对应的16*64*64个值的平均值\n",
    "var = torch.var(inputs, [0,2,3], keepdim=True)\n",
    "# print(mean)\n",
    "# print(var)\n",
    "bn = nn.BatchNorm2d(32)(inputs)\n",
    "bn_ = (inputs-mean)/torch.sqrt(var+1e-5)\n",
    "# print(bn[:,1,:,:])\n",
    "# print(bn_)\n",
    "print(torch.sum(torch.abs(bn-bn_)))    # 结果差距很小\n",
    "print(torch.sum(pdist(bn, bn_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Layer norm\n",
    "避开了batch维度的大小限制，直接按照batch维度对其他几个维度计算平均值和方差\n",
    "$$\n",
    "y=\\frac{x-E(x)}{\\sqrt{Var(x)+\\epsilon}}*\\gamma+\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9151, grad_fn=<SumBackward0>)\n",
      "tensor(1.0266, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand([16,32,64,64])\n",
    "mean = torch.mean(inputs, [1,2,3], keepdim=True) # 按batch维度即16那个维度，计算那个维度对应的32*64*64个值的平均值\n",
    "var = torch.var(inputs, [1,2,3], keepdim=True)\n",
    "ln = nn.LayerNorm([32,64,64])(inputs)\n",
    "ln_ = (inputs-mean)/torch.sqrt(var+1e-5)\n",
    "print(torch.sum(torch.abs(ln-ln_)))    # 结果差距很小\n",
    "print(torch.sum(pdist(ln, ln_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Instance norm\n",
    " 归一化的维度为[H,W]；根据N，C维度计算其他两个维度的平均值和方差，即计算H× W个数据的平均值和方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(221.6718)\n",
      "tensor(31.9452)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand([16,32,64,64])\n",
    "mean = torch.mean(inputs, [2,3], keepdim=True) # 按batch和channel维度即[16,32]那个维度，计算那个维度对应的64*64个值的平均值\n",
    "var = torch.var(inputs, [2,3], keepdim=True)\n",
    "In = nn.InstanceNorm2d(32)(inputs)\n",
    "In_ = (inputs-mean)/torch.sqrt(var+1e-5)\n",
    "print(torch.sum(torch.abs(In-In_)))    # 结果差距很小\n",
    "print(torch.sum(pdist(In, In_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Group norm\n",
    "介于LN和IN之间，其首先将channel分为许多组（group），对每一组做归一化，即先将feature的维度由[N, C, H, W]reshape为[N, G，C//G , H, W]，归一化的维度为[C//G , H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.4126, grad_fn=<SumBackward0>)\n",
      "tensor(31.9452)\n"
     ]
    }
   ],
   "source": [
    "groups = 8\n",
    "inputs = torch.rand([16,32,64,64])\n",
    "inputs_ = inputs.reshape([16, groups, 32//groups, 64, 64])\n",
    "mean = torch.mean(inputs_, [2,3,4], keepdim=True) # 按batch和group维度即[16,8]那个维度，计算那个维度对应的4*64*64个值的平均值\n",
    "var = torch.var(inputs_, [2,3,4], keepdim=True)\n",
    "gn = nn.GroupNorm(groups, 32)(inputs)\n",
    "gn_ = (inputs_-mean)/torch.sqrt(var+1e-5)\n",
    "gn_ = gn_.reshape([16, 32, 64,64])\n",
    "print(torch.sum(torch.abs(gn-gn_)))    # 结果差距很小\n",
    "print(torch.sum(pdist(In, In_)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchNorm是在batch上，对小batchsize效果不好；\n",
    "layerNorm在通道方向上，主要对RNN作用明显；\n",
    "instanceNorm在图像像素上，用在风格化迁移；\n",
    "GroupNorm将channel分组，然后再做归一化, 在batchsize<16的时候, 可以使用这种归一化；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
