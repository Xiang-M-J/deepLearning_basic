{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "def print_(model):\n",
    "    for name, params in model.named_parameters():\n",
    "        print('-->name:', name)\n",
    "        print('-->para:', params)\n",
    "        # print('-->grad_requires:', params.requires_grad)\n",
    "        print('-->grad_value:', params.grad)\n",
    "        print(\"===\")\n",
    "\n",
    "\n",
    "class simpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding='same', bias=False)\n",
    "        self.linear = nn.Linear(in_features=4, out_features=4, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.rand([4, 1, 2, 2])\n",
    "y = torch.rand([4, 4])\n",
    "x1 = torch.rand([4, 1, 2, 2])\n",
    "y1 = torch.rand([4, 4], requires_grad=True)\n",
    "model = simpleModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model_dict = copy.deepcopy(model.state_dict())\n",
    "optimizer1 = torch.optim.Adam(model.conv.parameters(), lr=1e-3)    # 只更新conv的参数\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原参数\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: None\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: None\n",
      "===\n",
      "梯度清零\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: None\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: None\n",
      "===\n",
      "反向传播\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0124,  0.0935,  0.1211],\n",
      "          [ 0.0403,  0.1514,  0.1282],\n",
      "          [ 0.0226,  0.0079, -0.0032]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n",
      "参数更新\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0238,  0.1653,  0.0978],\n",
      "          [ 0.1476,  0.2079,  0.2982],\n",
      "          [-0.0493, -0.0208,  0.2785]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0124,  0.0935,  0.1211],\n",
      "          [ 0.0403,  0.1514,  0.1282],\n",
      "          [ 0.0226,  0.0079, -0.0032]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1388,  0.1019, -0.2794,  0.3279],\n",
      "        [-0.2000,  0.0865,  0.4065,  0.2311],\n",
      "        [-0.2016, -0.0273, -0.2603,  0.0752],\n",
      "        [ 0.0099, -0.3525,  0.4254,  0.1772]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "loss1 = loss_fn(out, y)\n",
    "print(\"原参数\")\n",
    "print_(model)\n",
    "optimizer.zero_grad()\n",
    "print(\"梯度清零\")\n",
    "print_(model)\n",
    "loss1.backward()\n",
    "print(\"反向传播\")\n",
    "print_(model)\n",
    "optimizer.step()  # step后模型参数才会更新\n",
    "print(\"参数更新\")\n",
    "print_(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再次加载原模型\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0124,  0.0935,  0.1211],\n",
      "          [ 0.0403,  0.1514,  0.1282],\n",
      "          [ 0.0226,  0.0079, -0.0032]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n",
      "梯度清零\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n",
      "再次反向传播\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n",
      "再次更新\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_dict)\n",
    "print(\"再次加载原模型\")\n",
    "print_(model)           # 只加载了参数 梯度没变\n",
    "loss2 = loss_fn(y1, y)\n",
    "optimizer1.zero_grad()  # 只会清除conv的参数梯度\n",
    "print(\"梯度清零\")\n",
    "print_(model)\n",
    "loss2.backward()    # 反向传播会计算网络中所有的参数梯度，但由于没有经过模型计算，所以没有模型梯度计算\n",
    "print(\"再次反向传播\")\n",
    "print_(model)\n",
    "optimizer1.step()   # optimizer1 只根据conv的参数梯度来更新conv的参数\n",
    "print(\"再次更新\")\n",
    "print_(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再次加载原模型\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n",
      "参数清零\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.0397, -0.0126, -0.0421, -0.0322],\n",
      "        [ 0.1288,  0.0422,  0.1439,  0.1035],\n",
      "        [-0.0876, -0.0290, -0.0951, -0.0706],\n",
      "        [-0.0015, -0.0005, -0.0067, -0.0006]])\n",
      "===\n",
      "再次反向传播\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0242,  0.2385,  0.1565],\n",
      "          [ 0.0344,  0.1328,  0.1689],\n",
      "          [ 0.0174,  0.0279, -0.0100]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.1014, -0.0382, -0.0966, -0.0670],\n",
      "        [ 0.3467,  0.1282,  0.3387,  0.2350],\n",
      "        [-0.2324, -0.0883, -0.2226, -0.1565],\n",
      "        [-0.0129, -0.0017, -0.0195, -0.0115]])\n",
      "===\n",
      "再次更新\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0241,  0.1656,  0.0980],\n",
      "          [ 0.1479,  0.2081,  0.2985],\n",
      "          [-0.0490, -0.0205,  0.2783]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0242,  0.2385,  0.1565],\n",
      "          [ 0.0344,  0.1328,  0.1689],\n",
      "          [ 0.0174,  0.0279, -0.0100]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.1014, -0.0382, -0.0966, -0.0670],\n",
      "        [ 0.3467,  0.1282,  0.3387,  0.2350],\n",
      "        [-0.2324, -0.0883, -0.2226, -0.1565],\n",
      "        [-0.0129, -0.0017, -0.0195, -0.0115]])\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_dict)\n",
    "print(\"再次加载原模型\")\n",
    "print_(model)\n",
    "out1 = model(x1)\n",
    "loss2 = loss_fn(out1, y)\n",
    "optimizer1.zero_grad()\n",
    "print(\"参数清零\")\n",
    "print_(model)\n",
    "loss2.backward()    # 反向传播会计算网络中所有的参数梯度\n",
    "print(\"再次反向传播\")\n",
    "print_(model)\n",
    "optimizer1.step()   # optimizer1 只根据conv的参数梯度来更新conv的参数\n",
    "print(\"再次更新\")\n",
    "print_(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再次加载原模型\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0242,  0.2385,  0.1565],\n",
      "          [ 0.0344,  0.1328,  0.1689],\n",
      "          [ 0.0174,  0.0279, -0.0100]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.1014, -0.0382, -0.0966, -0.0670],\n",
      "        [ 0.3467,  0.1282,  0.3387,  0.2350],\n",
      "        [-0.2324, -0.0883, -0.2226, -0.1565],\n",
      "        [-0.0129, -0.0017, -0.0195, -0.0115]])\n",
      "===\n",
      "参数不清零\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0242,  0.2385,  0.1565],\n",
      "          [ 0.0344,  0.1328,  0.1689],\n",
      "          [ 0.0174,  0.0279, -0.0100]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.1014, -0.0382, -0.0966, -0.0670],\n",
      "        [ 0.3467,  0.1282,  0.3387,  0.2350],\n",
      "        [-0.2324, -0.0883, -0.2226, -0.1565],\n",
      "        [-0.0129, -0.0017, -0.0195, -0.0115]])\n",
      "===\n",
      "再次反向传播\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0248,  0.1663,  0.0988],\n",
      "          [ 0.1486,  0.2089,  0.2992],\n",
      "          [-0.0483, -0.0198,  0.2775]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0483,  0.4770,  0.3131],\n",
      "          [ 0.0687,  0.2657,  0.3379],\n",
      "          [ 0.0349,  0.0559, -0.0200]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.1631, -0.0637, -0.1512, -0.1019],\n",
      "        [ 0.5646,  0.2142,  0.5335,  0.3666],\n",
      "        [-0.3772, -0.1476, -0.3501, -0.2424],\n",
      "        [-0.0244, -0.0029, -0.0323, -0.0223]])\n",
      "===\n",
      "再次更新\n",
      "-->name: conv.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[[[ 0.0240,  0.1655,  0.0979],\n",
      "          [ 0.1478,  0.2080,  0.2984],\n",
      "          [-0.0491, -0.0206,  0.2783]]]], requires_grad=True)\n",
      "-->grad_value: tensor([[[[ 0.0483,  0.4770,  0.3131],\n",
      "          [ 0.0687,  0.2657,  0.3379],\n",
      "          [ 0.0349,  0.0559, -0.0200]]]])\n",
      "===\n",
      "-->name: linear.weight\n",
      "-->para: Parameter containing:\n",
      "tensor([[-0.1398,  0.1009, -0.2804,  0.3269],\n",
      "        [-0.1990,  0.0875,  0.4075,  0.2321],\n",
      "        [-0.2026, -0.0283, -0.2613,  0.0742],\n",
      "        [ 0.0089, -0.3535,  0.4244,  0.1762]], requires_grad=True)\n",
      "-->grad_value: tensor([[-0.1631, -0.0637, -0.1512, -0.1019],\n",
      "        [ 0.5646,  0.2142,  0.5335,  0.3666],\n",
      "        [-0.3772, -0.1476, -0.3501, -0.2424],\n",
      "        [-0.0244, -0.0029, -0.0323, -0.0223]])\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(model_dict)\n",
    "print(\"再次加载原模型\")\n",
    "print_(model)\n",
    "out1 = model(x1)\n",
    "loss2 = loss_fn(out1, y)\n",
    "# optimizer1.zero_grad()\n",
    "print(\"参数不清零\")   # 不清零后\n",
    "print_(model)\n",
    "loss2.backward()    # 反向传播会计算网络中所有的参数梯度，由于不清零，所以此时的模型梯度为清零时的两倍\n",
    "print(\"再次反向传播\")\n",
    "print_(model)\n",
    "optimizer1.step()   # optimizer1 只根据conv的参数梯度来更新conv的参数\n",
    "print(\"再次更新\")\n",
    "print_(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "总结：pytorch的更新机制，loss先通过反向传播计算出所以经过正向传播的网络的梯度，优化器再通过step()将梯度应用到优化器指定参数的更新上"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
